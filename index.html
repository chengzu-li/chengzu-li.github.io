<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="internships.html">Internships&nbsp;&amp;&nbsp;Visit</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-item"><a href="media_presentation.html">Media&nbsp;&amp;&nbsp;Presentations</a></div>
<div class="menu-item"><a href="assets/CV_PhD_Chengzu.pdf">CV</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h2><span style="color:black;font-size:24pt;font-family:Songti SC"><b>Chengzu Li</b></span><br /></h2>
<table class="imgtable"><tr><td>
<img src="assets/bio.jpg" alt="alt text" width="400px" height="300px" />&nbsp;</td>
<td align="left"><p><br />
PhD student in Computation, Cognition and Language<br />
<a href="https://ltl.mmll.cam.ac.uk/">Language Technology Lab</a><br />
<a href="https://www.cam.ac.uk/">University of Cambridge</a></p>
<p>[<a href="mailto:cl917@cam.ac.uk">Email</a>]
[<a href="https://github.com/chengzu-li">Github</a>] [<a href="https://scholar.google.com/citations?user=t_Bwt70AAAAJ">Google Scholar</a>] [<a href="https://www.semanticscholar.org/author/Chengzu-Li/2155795167">Semantic Scholar</a>] [<a href="assets/CV_PhD_Chengzu.pdf">CV</a>]</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a second-year PhD student in Computation, Cognition and Language at <a href="https://ltl.mmll.cam.ac.uk/">Language Technology Lab</a> in the University of Cambridge, supervised by <a href="https://sites.google.com/site/ivanvulic/">Dr. Ivan Vulić</a> and <a href="https://sergebelongie.github.io/">Prof. Serge Belongie</a>.
My research is supported by Cambridge Trust.
I am a member of <a href="https://www.jesus.cam.ac.uk/">Jesus College</a>.</p>
<p>Before starting my PhD, I received my MPhil degree in Advanced Computer Science at the Department of Computer Science, supervised by <a href="https://www.cl.cam.ac.uk/~sht25/">Prof. Simone Teufel</a>, supported by Cambridge Trust.
I was an undergraduate student in Automation at <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a>.</p>
<h2>Research</h2>
<p>My research interests include language grounding and multimodal reasoning (eg. image, structural knowledge, etc.).
I am currently exploring and focusing on the topics below:</p>
<ul>
<li><p>Spatial reasoning: evaluation and improvements [<a href="https://topviewrs.github.io/">TopViewRS</a>] [<a href="https://arxiv.org/abs/2504.15037">New Recipes</a>]</p>
</li>
<li><p>Multimodal reasoning: paradigm and methods [<a href="https://arxiv.org/abs/2501.07542v1">MVoT</a>] [<a href="https://arxiv.org/abs/2505.11409">Visual Planning</a>]</p>
</li>
</ul>
<p>I'm also interested in the potential downstream application scenarios of multimodal reasoning.</p>
<h2>Education</h2>
<ul>
<li><p>PhD (Probationary) in Computation, Cognition and Language, <a href="https://ltl.mmll.cam.ac.uk/">Language Technology Lab</a>, University of Cambridge, 2023 - now</p>
</li>
<li><p>Master of Philosophy in Advanced Computer Science, University of Cambridge, 2022 - 2023</p>
<ul>
<li><p>Graduation with Distinction</p>
</li></ul>
</li>
<li><p>Bachelor of Engineering in Automation, Xi'an Jiaotong University, 2018 - 2022</p>
<ul>
<li><p>Graduate with Distinction (91.88/100)</p>
</li>
<li><p>Minor in Fintech (China Construction Bank - XJTU Fintech Elite Class)</p>
</li>
</ul>

</li>
</ul>
<h2>Publications </h2>
<p>(*: equal contribution)</p>
<p><b>Preprints</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.11409">Visual Planning: Let's Think Only with Images</a> <br />
Yi Xu*, <b>Chengzu Li</b>*, Han Zhou*, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić <br />
arXiv. [<a href="https://github.com/yix8/VisualPlanning">code</a>] [<a href="https://mp.weixin.qq.com/s/KXx1t3jIlhLWu0rlVoQWNA">机器之心</a>] [<a href="https://mp.weixin.qq.com/s/FNeGEV7Vccfga53GxC9MMg">量子位</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2504.15037">Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes</a> <br />
Huanyu Zhang*, <b>Chengzu Li</b>*, Wenshan Wu, Shaoguang Mao, Yan Xia, Ivan Vulić, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei <br />
arXiv. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.23912">Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation</a> <br />
Caiqi Zhang*, Xiaochen Zhu*, <b>Chengzu Li</b>, Nigel Collier, Andreas Vlachos <br />
arXiv. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.12568">Enriching Patent Claim Generation with European Patent Dataset</a> <br />
Lekang Jiang, <b>Chengzu Li</b>, Stephan Goetz <br />
arXiv. </p>
</li>
</ul>
<p><b>2025</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2501.07542v1">Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</a> <br />
<b>Chengzu Li</b>*, Wenshan Wu*, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei <br />
ICML 2025. [<a href="https://github.com/chengzu-li/MVoT">code</a>] [<a href="https://spectrum.ieee.org/visual-reasoning-in-ai">IEEE Spectrum</a>] [<a href="https://twimlai.com/podcast/twimlai/imagine-while-reasoning-in-space-multimodal-visualization-of-thought/">TWIML Podcast</a>] [<a href="https://mp.weixin.qq.com/s/JwXYrDxiajnv0tNUuOPYFg">新智元</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.13772">Large Language Models are Miscalibrated In-Context Learners</a><br />
<b>Chengzu Li</b>, Han Zhou, Goran Glavaš, Anna Korhonen, Ivan Vulić. <br />
ACL 2025, Findings. [<a href="https://github.com/cambridgeltl/ensembled-sicl">code</a>]</p>
</li>
</ul>
<p><b>2024</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2406.02537">TopViewRS: Vision-Language Models as Top-View Spatial Reasoners</a> <br />
<b>Chengzu Li</b>*, Caiqi Zhang*, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić <br />
EMNLP 2024, main (oral). [<a href="https://topviewrs.github.io/">project website</a>] [<a href="https://github.com/cambridgeltl/topviewrs">code</a>] [<a href="https://huggingface.co/datasets/chengzu/topviewrs">data</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2403.19603">Semantic Map-based Generation of Navigation Instructions</a><br />
<b>Chengzu Li</b>, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla, Svetlana Stoyanchev.<br />
COLING-LREC 2024. [<a href="https://github.com/chengzu-li/VLGen">code</a>]</p>
</li>
</ul>
<p><b>2023</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.13917">Generating Data for Symbolic Language with Large Language Models</a><br />
Jiacheng Ye, <b>Chengzu Li</b>, Lingpeng Kong, Tao Yu.<br />
EMNLP 2023, main. [<a href="https://github.com/HKUNLP/SymGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.02875">Binding Language Models in Symbolic Languages</a><br />
Zhoujun Cheng, Tianbao Xie, Peng Shi, <b>Chengzu Li</b>, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu. <br />
ICLR 2023 (spotlight). [<a href="https://github.com/HKUNLP/Binder">code</a>]</p>
</li>
</ul>
<p><b>2022</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2201.05966">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</a><br />
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, <b>Chengzu Li</b>, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu. <br />
EMNLP 2022, main (oral). [<a href="https://github.com/hkunlp/unifiedskg">code</a>]</p>
</li>
</ul>
<h2>Internships</h2>
<ul>
<li><p>Aug. 2025 - Nov. 2025
<br />Visiting Student, Belongie Lab, Pioneer Center for AI.
<br />Mentor: <a href="https://scholar.google.com/citations?user=chD5XxkAAAAJ&amp;hl=en">Prof. Serge Belongie</a>.
<br />Research on Multimodal Reasoning.</p>
</li>
</ul>
<ul>
<li><p>Jun. 2024 - Dec. 2024
<br />Research Intern, <a href="https://www.microsoft.com/en-us/research/group/general-artificial-intelligence/">GenAI Group, Microsoft Research</a>.
<br />Mentor: <a href="https://scholar.google.com/citations?user=AwANOsQAAAAJ">Wenshan Wu</a>.
<br />Research on Multimodal Spatial Reasoning.</p>
</li>
</ul>
<ul>
<li><p>Jan. 2023 - July 2023
<br />Research Intern, <a href="https://www.toshiba.eu/pages/eu/Cambridge-Research-Laboratory/">Toshiba Cambridge Research</a>.
<br />Mentor: <a href="https://www.linkedin.com/in/svetlana-stoyanchev/?originalSubdomain=uk">Svetlana Stoyanchev</a>, <a href="https://sites.google.com/site/cschaozhang/">Chao Zhang</a>, <a href="https://scholar.google.co.uk/citations?user=xxd3HZsAAAAJ&amp;hl=en">Rama Sanand Doddipatla</a> and <a href="https://www.cl.cam.ac.uk/~sht25/">Prof. Simone Teufel</a>.
<br />Research on Generating Instructions for Robot Navigation as the MPhil thesis.</p>
</li>
</ul>
<ul>
<li><p>Oct. 2021 - Jan. 2022
<br />Research Intern, Shanghai AI Lab.
<br />Mentor: <a href="https://ikekonglp.github.io/">Lingpeng Kong</a>.
<br />Research on Structural Knowledge Grounding.</p>
</li>
</ul>
<ul>
<li><p>June 2021 - Oct. 2022
<br />Research Intern, <a href="https://hkunlp.github.io/">HKUNLP</a>.
<br />Mentor: <a href="https://taoyds.github.io/">Tao Yu</a> and <a href="https://ikekonglp.github.io/">Lingpeng Kong</a>.
<br />Worked on Structural Knowledge Grounding, Semantic Parsing and Neural-Symbolic Reasoning.</p>
</li>
</ul>
<ul>
<li><p>July 2020 - Aug. 2020
<br />Engineer Intern, Dianchu Technology.
<br />Worked on machine learning engineering.</p>
</li>
</ul>
<h2>Selected Honors &amp; Awards</h2>
<ul>
<li><p>Scholar of Jesus College, Cambridge (for outstanding academic performance), 2024.</p>
</li>
<li><p>PhD Scholarship, Cambridge Trust, 2023.</p>
</li>
<li><p>Master Scholarship, Cambridge Trust, 2022.</p>
</li>
<li><p>National Scholarship (1%), Ministry of Education of China, 2019.</p>
</li>
</ul>
<h2>Media Coverage and Presentations</h2>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/12UFPQxfwEHM6lScMME-JdWYsw-AuURsd8AG-L9MeLxg">Invited Talk at HKUST (GZ): Reason with Multimodal Minds in Space</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/KXx1t3jIlhLWu0rlVoQWNA">机器之心：只用图像也能思考，强化学习造就推理模型新范式！复杂场景规划能力Max</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FNeGEV7Vccfga53GxC9MMg">量子位：纯靠“脑补”图像，大模型推理准确率狂飙80%丨剑桥谷歌新研究</a></p>
</li>
<li><p><a href="https://twimlai.com/podcast/twimlai/imagine-while-reasoning-in-space-multimodal-visualization-of-thought/">The TWIML AI Podcast with Sam Charrington</a></p>
</li>
<li><p><a href="https://spectrum.ieee.org/visual-reasoning-in-ai">IEEE Spectrum, &lsquo;&lsquo;Thinking" Visually Boosts AI Problem Solving</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/JwXYrDxiajnv0tNUuOPYFg">新智元：直接可视化多模态推理过程</a></p>
</li>
<li><p><a href="https://www.youtube.com/watch?v=Y78aSGJNeXY">BMVA: Trustworthy Multimodal Learning with Foundation Models</a></p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
