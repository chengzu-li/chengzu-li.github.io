<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="internships.html">Internships</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-item"><a href="media_presentation.html">Media&nbsp;&amp;&nbsp;Presentations</a></div>
<div class="menu-item"><a href="assets/CV_PhD_Chengzu.pdf">CV</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h2>Publications </h2>
<p><br />
[<a href="https://scholar.google.com/citations?user=t_Bwt70AAAAJ">Google Scholar</a>] [<a href="https://www.semanticscholar.org/author/Chengzu-Li/2155795167">Semantic Scholar</a>]</p>
<p>(*: equal contribution)</p>
<p><b>Preprints</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.11409">Visual Planning: Let's Think Only with Images</a> <br />
Yi Xu*, <b>Chengzu Li</b>*, Han Zhou*, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić <br />
arXiv. [<a href="https://github.com/yix8/VisualPlanning">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2504.15037">Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes</a> <br />
Huanyu Zhang*, <b>Chengzu Li</b>*, Wenshan Wu, Shaoguang Mao, Yan Xia, Ivan Vulić, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei <br />
arXiv. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.23912">Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation</a> <br />
Caiqi Zhang*, Xiaochen Zhu*, <b>Chengzu Li</b>, Nigel Collier, Andreas Vlachos <br />
arXiv. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.12568">Enriching Patent Claim Generation with European Patent Dataset</a> <br />
Lekang Jiang, <b>Chengzu Li</b>, Stephan Goetz <br />
arXiv. </p>
</li>
</ul>
<p><b>2025</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2501.07542v1">Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</a> <br />
<b>Chengzu Li</b>*, Wenshan Wu*, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei <br />
ICML 2025. [<a href="https://github.com/chengzu-li/MVoT">code</a>] [<a href="https://spectrum.ieee.org/visual-reasoning-in-ai">IEEE Spectrum</a>] [<a href="https://twimlai.com/podcast/twimlai/imagine-while-reasoning-in-space-multimodal-visualization-of-thought/">TWIML Podcast</a>] [<a href="https://mp.weixin.qq.com/s/JwXYrDxiajnv0tNUuOPYFg">新智元</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.13772">Large Language Models are Miscalibrated In-Context Learners</a><br />
<b>Chengzu Li</b>, Han Zhou, Goran Glavaš, Anna Korhonen, Ivan Vulić. <br />
ACL 2025, Findings. [<a href="https://github.com/cambridgeltl/ensembled-sicl">code</a>]</p>
</li>
</ul>
<p><b>2024</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2406.02537">TopViewRS: Vision-Language Models as Top-View Spatial Reasoners</a> <br />
<b>Chengzu Li</b>*, Caiqi Zhang*, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić <br />
EMNLP 2024, main (oral). [<a href="https://topviewrs.github.io/">project website</a>] [<a href="https://github.com/cambridgeltl/topviewrs">code</a>] [<a href="https://huggingface.co/datasets/chengzu/topviewrs">data</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2403.19603">Semantic Map-based Generation of Navigation Instructions</a><br />
<b>Chengzu Li</b>, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla, Svetlana Stoyanchev.<br />
COLING-LREC 2024. [<a href="https://github.com/chengzu-li/VLGen">code</a>]</p>
</li>
</ul>
<p><b>2023</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.13917">Generating Data for Symbolic Language with Large Language Models</a><br />
Jiacheng Ye, <b>Chengzu Li</b>, Lingpeng Kong, Tao Yu.<br />
EMNLP 2023, main. [<a href="https://github.com/HKUNLP/SymGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.02875">Binding Language Models in Symbolic Languages</a><br />
Zhoujun Cheng, Tianbao Xie, Peng Shi, <b>Chengzu Li</b>, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu. <br />
ICLR 2023 (spotlight). [<a href="https://github.com/HKUNLP/Binder">code</a>]</p>
</li>
</ul>
<p><b>2022</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2201.05966">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</a><br />
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, <b>Chengzu Li</b>, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu. <br />
EMNLP 2022, main (oral). [<a href="https://github.com/hkunlp/unifiedskg">code</a></p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
