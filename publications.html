<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="internships.html">Internships</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-item"><a href="assets/cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h2>Publications </h2>
<p><br />
[<a href="https://scholar.google.com/citations?user=t_Bwt70AAAAJ">Google Scholar</a>] [<a href="https://www.semanticscholar.org/author/Chengzu-Li/2155795167">Semantic Scholar</a>]</p>
<p>(*: equal contribution)</p>
<p><b>Preprints</b></p>
<ul>
<li><p>ðŸ”¥<a href="https://drive.google.com/file/d/1COaDxaqHKYlYdfIHgudYeRqvLIfBv1NP/view?usp=sharing">Imagine while Reasoning in Space: Multimodal Visualization-of-Thoughtn</a><br />
<b>Chengzu Li</b>*, Wenshan Wu*, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan VuliÄ‡, Furu Wei <br />
arXiv. [<a href="https://drive.google.com/file/d/1COaDxaqHKYlYdfIHgudYeRqvLIfBv1NP/view?usp=sharing">pdf</a>] </p>
</li>
</ul>
<p><b>2024</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2406.02537">TopViewRS: Vision-Language Models as Top-View Spatial Reasoners</a><br />
<b>Chengzu Li</b>*, Caiqi Zhang*, Han Zhou, Nigel Collier, Anna Korhonen, Ivan VuliÄ‡ <br />
EMNLP 2024, main (oral). [<a href="https://topviewrs.github.io/">project website</a>] [<a href="https://arxiv.org/abs/2406.02537">pdf</a>] [<a href="https://github.com/cambridgeltl/topviewrs">code</a>] [<a href="https://huggingface.co/datasets/chengzu/topviewrs">data</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.13772">On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning</a><br />
<b>Chengzu Li</b>, Han Zhou, Goran GlavaÅ¡, Anna Korhonen, Ivan VuliÄ‡. <br />
ICLR 2024 Workshop on Reliable and Responsible Foundation Models. [<a href="https://arxiv.org/abs/2312.13772">pdf</a>] [<a href="https://github.com/cambridgeltl/ensembled-sicl">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2403.19603">Semantic Map-based Generation of Navigation Instructions</a><br />
<b>Chengzu Li</b>, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla, Svetlana Stoyanchev.<br />
COLING-LREC 2024. [<a href="https://arxiv.org/abs/2403.19603">pdf</a>] [<a href="https://github.com/chengzu-li/VLGen">code</a>]</p>
</li>
</ul>
<p><b>2023</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.13917">Generating Data for Symbolic Language with Large Language Models</a><br />
Jiacheng Ye, <b>Chengzu Li</b>, Lingpeng Kong, Tao Yu.<br />
EMNLP 2023. [<a href="https://arxiv.org/abs/2305.13917">pdf</a>] [<a href="https://github.com/HKUNLP/SymGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.02875">Binding Language Models in Symbolic Languages</a><br />
Zhoujun Cheng, Tianbao Xie, Peng Shi, <b>Chengzu Li</b>, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu. <br />
ICLR 2023 (spotlight). [<a href="https://arxiv.org/abs/2210.02875">pdf</a>] [<a href="https://github.com/HKUNLP/Binder">code</a>]</p>
</li>
</ul>
<p><b>2022</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2201.05966">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</a><br />
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, <b>Chengzu Li</b>, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu. <br />
EMNLP 2022, main (oral). [<a href="https://arxiv.org/abs/2201.05966">pdf</a>] [<a href="https://github.com/hkunlp/unifiedskg">code</a>]</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
