# jemdoc: menu{MENU}{index.html}, nofooter
\n
== {{<span style="color:black;font-size:24pt;font-family:Songti SC">}}*Chengzu Li*{{</span>}}\n

~~~
{}{img_left}{assets/bio.jpg}{alt text}{400}{300}
\n
PhD student in Computation, Cognition and Language\n
[https://ltl.mmll.cam.ac.uk/ Language Technology Lab]\n
[https://www.cam.ac.uk/ University of Cambridge]


\[[cl917@cam.ac.uk Email]\]
\[[https://github.com/chengzu-li Github]\] \[[https://scholar.google.com/citations?user=t_Bwt70AAAAJ Google Scholar]\] \[[https://www.semanticscholar.org/author/Chengzu-Li/2155795167 Semantic Scholar]\] \[[assets/CV_PhD_Chengzu.pdf CV]\]
~~~

== About me
I am a second-year PhD student in Computation, Cognition and Language at [https://ltl.mmll.cam.ac.uk/ Language Technology Lab] in the University of Cambridge, supervised by [https://sites.google.com/site/ivanvulic/ Dr. Ivan Vulić] and [https://sergebelongie.github.io/ Prof. Serge Belongie].
My research is supported by Cambridge Trust.
I am a member of [https://www.jesus.cam.ac.uk/ Jesus College].

Before starting my PhD, I received my MPhil degree in Advanced Computer Science at the Department of Computer Science, supervised by [https://www.cl.cam.ac.uk/~sht25/ Prof. Simone Teufel], supported by Cambridge Trust.
I was an undergraduate student in Automation at [http://en.xjtu.edu.cn/ Xi'an Jiaotong University].


== Research
My research interests include language grounding and multimodal reasoning (eg. image, structural knowledge, etc.).
I am currently exploring and focusing on the topics below:
- Spatial reasoning: evaluation and improvements \[[https://topviewrs.github.io/ TopViewRS]\] \[[https://arxiv.org/abs/2504.15037 New Recipes]\]
- Multimodal reasoning: paradigm and methods \[[https://arxiv.org/abs/2501.07542v1 MVoT]\] \[[https://arxiv.org/abs/2505.11409 Visual Planning]\]

I'm also interested in the potential downstream application scenarios of multimodal reasoning.

== Education
- PhD (Probationary) in Computation, Cognition and Language, [https://ltl.mmll.cam.ac.uk/ Language Technology Lab], University of Cambridge, 2023 \- now
- Master of Philosophy in Advanced Computer Science, University of Cambridge, 2022 \- 2023
-- Graduation with Distinction
- Bachelor of Engineering in Automation, Xi'an Jiaotong University, 2018 \- 2022
-- Graduate with Distinction (91.88/100)
-- Minor in Fintech (China Construction Bank \- XJTU Fintech Elite Class)


== Publications 
(\*: equal contribution)

*Preprints*

- [https://arxiv.org/abs/2505.11409 Visual Planning: Let's Think Only with Images] \n
Yi Xu\*, *Chengzu Li*\*, Han Zhou\*, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić \n
arXiv. \[[https://github.com/yix8/VisualPlanning code]\] \[[https://mp.weixin.qq.com/s/KXx1t3jIlhLWu0rlVoQWNA 机器之心]\] \[[https://mp.weixin.qq.com/s/FNeGEV7Vccfga53GxC9MMg 量子位]\]

- [https://arxiv.org/abs/2504.15037 Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes] \n
Huanyu Zhang\*, *Chengzu Li*\*, Wenshan Wu, Shaoguang Mao, Yan Xia, Ivan Vulić, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei \n
arXiv. 

- [https://arxiv.org/abs/2505.23912 Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation] \n
Caiqi Zhang\*, Xiaochen Zhu\*, *Chengzu Li*, Nigel Collier, Andreas Vlachos \n
arXiv. 

- [https://arxiv.org/abs/2505.12568 Enriching Patent Claim Generation with European Patent Dataset] \n
Lekang Jiang, *Chengzu Li*, Stephan Goetz \n
arXiv. 


*2025*
- [https://arxiv.org/abs/2501.07542v1 Imagine while Reasoning in Space: Multimodal Visualization-of-Thought] \n
*Chengzu Li*\*, Wenshan Wu\*, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei \n
ICML 2025. \[[https://github.com/chengzu-li/MVoT code]\] \[[https://spectrum.ieee.org/visual-reasoning-in-ai IEEE Spectrum]\] \[[https://twimlai.com/podcast/twimlai/imagine-while-reasoning-in-space-multimodal-visualization-of-thought/ TWIML Podcast]\] \[[https://mp.weixin.qq.com/s/JwXYrDxiajnv0tNUuOPYFg 新智元]\]

- [https://arxiv.org/abs/2312.13772 Large Language Models are Miscalibrated In-Context Learners]\n
*Chengzu Li*, Han Zhou, Goran Glavaš, Anna Korhonen, Ivan Vulić. \n
ACL 2025, Findings. \[[https://github.com/cambridgeltl/ensembled-sicl code]\]

*2024*
- [https://arxiv.org/abs/2406.02537 TopViewRS: Vision-Language Models as Top-View Spatial Reasoners] \n
*Chengzu Li*\*, Caiqi Zhang\*, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić \n
EMNLP 2024, main (oral). \[[https://topviewrs.github.io/ project website]\] \[[https://github.com/cambridgeltl/topviewrs code]\] \[[https://huggingface.co/datasets/chengzu/topviewrs data]\]

- [https://arxiv.org/abs/2403.19603 Semantic Map-based Generation of Navigation Instructions]\n
*Chengzu Li*, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla, Svetlana Stoyanchev.\n
COLING-LREC 2024. \[[https://github.com/chengzu-li/VLGen code]\]

*2023*

- [https://arxiv.org/abs/2305.13917 Generating Data for Symbolic Language with Large Language Models]\n
Jiacheng Ye, *Chengzu Li*, Lingpeng Kong, Tao Yu.\n
EMNLP 2023, main. \[[https://github.com/HKUNLP/SymGen code]\]

- [https://arxiv.org/abs/2210.02875 Binding Language Models in Symbolic Languages]\n
Zhoujun Cheng, Tianbao Xie, Peng Shi, *Chengzu Li*, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu. \n
ICLR 2023 (spotlight). \[[https://github.com/HKUNLP/Binder code]\]

*2022*

- [https://arxiv.org/abs/2201.05966 UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models]\n
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, *Chengzu Li*, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu. \n
EMNLP 2022, main (oral). \[[https://github.com/hkunlp/unifiedskg code]\]


== Internships
- Aug. 2025 - Nov. 2025
\nVisiting Student, Belongie Lab, Pioneer Center for AI.
\nMentor: [https://scholar.google.com/citations?user=chD5XxkAAAAJ&hl=en Prof. Serge Belongie].
\nResearch on Multimodal Reasoning.

- Jun. 2024 - Dec. 2024
\nResearch Intern, [https://www.microsoft.com/en-us/research/group/general-artificial-intelligence/ GenAI Group, Microsoft Research].
\nMentor: [https://scholar.google.com/citations?user=AwANOsQAAAAJ Wenshan Wu].
\nResearch on Multimodal Spatial Reasoning.

- Jan. 2023 - July 2023
\nResearch Intern, [https://www.toshiba.eu/pages/eu/Cambridge-Research-Laboratory/ Toshiba Cambridge Research].
\nMentor: [https://www.linkedin.com/in/svetlana-stoyanchev/?originalSubdomain=uk Svetlana Stoyanchev], [https://sites.google.com/site/cschaozhang/ Chao Zhang], [https://scholar.google.co.uk/citations?user=xxd3HZsAAAAJ&hl=en Rama Sanand Doddipatla] and [https://www.cl.cam.ac.uk/~sht25/ Prof. Simone Teufel].
\nResearch on Generating Instructions for Robot Navigation as the MPhil thesis.

- Oct. 2021 - Jan. 2022
\nResearch Intern, Shanghai AI Lab.
\nMentor: [https://ikekonglp.github.io/ Lingpeng Kong].
\nResearch on Structural Knowledge Grounding.

- June 2021 - Oct. 2022
\nResearch Intern, [https://hkunlp.github.io/ HKUNLP].
\nMentor: [https://taoyds.github.io/ Tao Yu] and [https://ikekonglp.github.io/ Lingpeng Kong].
\nWorked on Structural Knowledge Grounding, Semantic Parsing and Neural-Symbolic Reasoning.

- July 2020 - Aug. 2020
\nEngineer Intern, Dianchu Technology.
\nWorked on machine learning engineering.


== Selected Honors & Awards
- Scholar of Jesus College, Cambridge (for outstanding academic performance), 2024.
- PhD Scholarship, Cambridge Trust, 2023.
- Master Scholarship, Cambridge Trust, 2022.
- National Scholarship (1\%), Ministry of Education of China, 2019.


== Media Coverage and Presentations
- [https://docs.google.com/presentation/d/12UFPQxfwEHM6lScMME-JdWYsw-AuURsd8AG-L9MeLxg/edit?slide=id.g20636aa3674_0_0#slide=id.g20636aa3674_0_0 Invited Talk at HKUST (GZ): Reason with Multimodal Minds in Space]
- [https://mp.weixin.qq.com/s/KXx1t3jIlhLWu0rlVoQWNA 机器之心：只用图像也能思考，强化学习造就推理模型新范式！复杂场景规划能力Max]
- [https://mp.weixin.qq.com/s/FNeGEV7Vccfga53GxC9MMg 量子位：纯靠“脑补”图像，大模型推理准确率狂飙80%丨剑桥谷歌新研究]
- [https://twimlai.com/podcast/twimlai/imagine-while-reasoning-in-space-multimodal-visualization-of-thought/ The TWIML AI Podcast with Sam Charrington]
- [https://spectrum.ieee.org/visual-reasoning-in-ai IEEE Spectrum, ``Thinking" Visually Boosts AI Problem Solving]
- [https://mp.weixin.qq.com/s/JwXYrDxiajnv0tNUuOPYFg 新智元：直接可视化多模态推理过程]
- [https://www.youtube.com/watch?v=Y78aSGJNeXY BMVA: Trustworthy Multimodal Learning with Foundation Models]
